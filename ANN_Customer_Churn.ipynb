{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN Customer Churn",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrQAeShlldO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries we will need \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSRhnUAhlfM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f43bc2a7-4707-473c-c817-f0e98905a6d0"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7gvVPkmlfVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the dataset \n",
        "data = pd.read_csv(\"Churn_Modelling.csv\")\n",
        "\n",
        "# X is the matrix, we use every row (:) and then the columns AFTER the 3rd column and all but the last hence data.iloc[:,3:-1]\n",
        "X = data.iloc[:,3:-1].values\n",
        "\n",
        "# y is the dependent variable or the variable we are predicting and we import every row (:), and we only import the last column hence [:,-1]\n",
        "y = data.iloc[:,-1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFZh6g5Bowtl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "5661eb12-11d7-482d-94bc-b0a9667189d7"
      },
      "source": [
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[619 'France' 'Female' ... 1 1 101348.88]\n",
            " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
            " [502 'France' 'Female' ... 1 0 113931.57]\n",
            " ...\n",
            " [709 'France' 'Female' ... 0 1 42085.58]\n",
            " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
            " [792 'France' 'Female' ... 1 0 38190.78]]\n",
            "[1 0 1 ... 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l34xWAR8o6dg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "edffc538-f1c8-43bc-d95a-ec41e06670d4"
      },
      "source": [
        "# We have a few cagtegorical variables, so we need to encode them (country and gender), we also need to import a few more things\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encoding gender\n",
        "le = LabelEncoder()\n",
        "X[:,2] = le.fit_transform(X[:,2])\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[619 'France' 0 ... 1 1 101348.88]\n",
            " [608 'Spain' 0 ... 0 1 112542.58]\n",
            " [502 'France' 0 ... 1 0 113931.57]\n",
            " ...\n",
            " [709 'France' 0 ... 0 1 42085.58]\n",
            " [772 'Germany' 1 ... 1 0 92888.52]\n",
            " [792 'France' 0 ... 1 0 38190.78]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4P5tFhmr9x8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "a5297eae-5665-41ae-a93b-d8e44464b8cc"
      },
      "source": [
        "# Depending on the data we have, we might run into situations where, after label encoding, we might confuse our model into thinking that a column has data with\n",
        "# some kind of order or hierarchy when we clearly don’t have it. To avoid this, we ‘OneHotEncode’ that column. What one hot encoding does is, it takes a column\n",
        "# which has categorical data, which has been label encoded and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, \n",
        "# depending on which column has what value. In our example, we’ll get four new columns, one for each country — Japan, U.S, India, and China.\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer([('encoder', OneHotEncoder(), [1])], remainder = 'passthrough')\n",
        "X = np.array(ct.fit_transform(X))\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
            " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
            " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
            " ...\n",
            " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
            " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
            " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtS1jfyQtfHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa1LsZLHuMBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale the train and test set \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F_mlAbDvBFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sequential class allows us to build out an ANN in a sequential matter adding layers one after the other from the input all the way to the output\n",
        "# Remember keras was kinda imported into tf so we call it from the tf library\n",
        "ann = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiVbr-r8wAon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will use the dense class in order to add our first input and hidden layers\n",
        "ann.add(tf.keras.layers.Dense(units = 6, activation= 'relu'))\n",
        "\n",
        "# We use the same line of code to add another hidden layer\n",
        "ann.add(tf.keras.layers.Dense(units = 6, activation= 'relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5t1vklixuDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we need to add the output layer to the model we do almost the same thing, but we change the number of units and the activation function\n",
        "# The number of units is one because we are predicting a binary variable\n",
        "# We don't want to have the rectifier fxn (relu) to be predicting the dependent var so we change to sigmoid bc it gives us the probability that a customer decides\n",
        "# to leave the bank (gives us the probability of a certain outcome) sigmoid is probably the most common fxn used for output layers that are binary\n",
        "ann.add(tf.keras.layers.Dense(units = 1, activation= 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBX5YOL70KbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we have to compile the ANN, the parameter for the compile method are the optimizer, the loss fxn, and the metric\n",
        "# Optimizers that can perform Stochastic Gradient Descent (SGD) is usually the best performing so we use adam\n",
        "# Loss fxn for binary outcomes will always be binary_crossentropy otherwise if the outcome is not binary we use categorical_crossentropy\n",
        "# Also, if we have more than 2 categories, the activation fxn for the output layer should be softmax\n",
        "ann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0DGHo8K3CWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c3b9b712-f228-4bd8-99a0-398a5d8e73a2"
      },
      "source": [
        "# Now we need to train the model, the batch size is more efficient and has better performance when training an ANN (Batch Learning), it's not comparing results one \n",
        "# by one, but it's doing it for a whole batch instead and the classic batch size is 32 so we use 32, but it is a hyperparameter so we can tune it for better \n",
        "# performance if needed. The last parameter is epochs (I like to call it a runthrough), which is the number of times we essentially train the model\n",
        "# (don't use small # for epochs bc it wont be as accurate)\n",
        "\n",
        "ann.fit(X_train, y_train, batch_size=32, epochs = 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.5696 - accuracy: 0.7440\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4597 - accuracy: 0.7972\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 0s 984us/step - loss: 0.4359 - accuracy: 0.8065\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 0s 947us/step - loss: 0.4252 - accuracy: 0.8127\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4160 - accuracy: 0.8211\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.4054 - accuracy: 0.8285\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 0s 946us/step - loss: 0.3933 - accuracy: 0.8385\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3806 - accuracy: 0.8445\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 0s 960us/step - loss: 0.3697 - accuracy: 0.8486\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 0s 937us/step - loss: 0.3620 - accuracy: 0.8526\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 0s 930us/step - loss: 0.3564 - accuracy: 0.8545\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 0s 980us/step - loss: 0.3528 - accuracy: 0.8559\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 0s 934us/step - loss: 0.3508 - accuracy: 0.8561\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 0s 981us/step - loss: 0.3489 - accuracy: 0.8566\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3477 - accuracy: 0.8572\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 0s 908us/step - loss: 0.3466 - accuracy: 0.8562\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8561\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 0s 933us/step - loss: 0.3449 - accuracy: 0.8579\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 0s 911us/step - loss: 0.3444 - accuracy: 0.8572\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 0s 986us/step - loss: 0.3435 - accuracy: 0.8575\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 0s 992us/step - loss: 0.3431 - accuracy: 0.8590\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 0s 983us/step - loss: 0.3425 - accuracy: 0.8593\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 0s 992us/step - loss: 0.3421 - accuracy: 0.8601\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 0s 935us/step - loss: 0.3417 - accuracy: 0.8591\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 0s 967us/step - loss: 0.3414 - accuracy: 0.8600\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 0s 968us/step - loss: 0.3410 - accuracy: 0.8609\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 0s 998us/step - loss: 0.3405 - accuracy: 0.8605\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 0s 970us/step - loss: 0.3403 - accuracy: 0.8608\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 0s 959us/step - loss: 0.3401 - accuracy: 0.8604\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 0s 989us/step - loss: 0.3394 - accuracy: 0.8610\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 0s 969us/step - loss: 0.3396 - accuracy: 0.8612\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 0s 921us/step - loss: 0.3391 - accuracy: 0.8597\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 0s 916us/step - loss: 0.3387 - accuracy: 0.8614\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3384 - accuracy: 0.8595\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 0s 915us/step - loss: 0.3384 - accuracy: 0.8612\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 0s 973us/step - loss: 0.3378 - accuracy: 0.8614\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 0s 946us/step - loss: 0.3378 - accuracy: 0.8609\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 0s 976us/step - loss: 0.3374 - accuracy: 0.8608\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 0s 935us/step - loss: 0.3372 - accuracy: 0.8610\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 0s 923us/step - loss: 0.3369 - accuracy: 0.8621\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 0s 942us/step - loss: 0.3369 - accuracy: 0.8608\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 0s 973us/step - loss: 0.3365 - accuracy: 0.8622\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 0s 970us/step - loss: 0.3366 - accuracy: 0.8605\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 0s 977us/step - loss: 0.3363 - accuracy: 0.8616\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 0s 990us/step - loss: 0.3356 - accuracy: 0.8626\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 0s 939us/step - loss: 0.3360 - accuracy: 0.8614\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 0s 903us/step - loss: 0.3357 - accuracy: 0.8608\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 0s 940us/step - loss: 0.3355 - accuracy: 0.8635\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8589\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 0s 900us/step - loss: 0.3353 - accuracy: 0.8626\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3349 - accuracy: 0.8630\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 0s 967us/step - loss: 0.3349 - accuracy: 0.8635\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 0s 950us/step - loss: 0.3348 - accuracy: 0.8624\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 0s 912us/step - loss: 0.3347 - accuracy: 0.8621\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 0s 944us/step - loss: 0.3346 - accuracy: 0.8634\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 0s 962us/step - loss: 0.3348 - accuracy: 0.8622\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 0s 948us/step - loss: 0.3345 - accuracy: 0.8634\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 0s 955us/step - loss: 0.3342 - accuracy: 0.8636\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 0s 979us/step - loss: 0.3341 - accuracy: 0.8639\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 0s 966us/step - loss: 0.3344 - accuracy: 0.8630\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 0s 924us/step - loss: 0.3342 - accuracy: 0.8644\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 0s 916us/step - loss: 0.3343 - accuracy: 0.8619\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8624\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 0s 962us/step - loss: 0.3341 - accuracy: 0.8631\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 0s 976us/step - loss: 0.3338 - accuracy: 0.8611\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 0s 928us/step - loss: 0.3335 - accuracy: 0.8629\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 0s 971us/step - loss: 0.3339 - accuracy: 0.8631\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 0s 922us/step - loss: 0.3339 - accuracy: 0.8627\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 0s 938us/step - loss: 0.3334 - accuracy: 0.8627\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 0s 994us/step - loss: 0.3337 - accuracy: 0.8631\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3334 - accuracy: 0.8635\n",
            "Epoch 72/100\n",
            "250/250 [==============================] - 0s 934us/step - loss: 0.3333 - accuracy: 0.8621\n",
            "Epoch 73/100\n",
            "250/250 [==============================] - 0s 927us/step - loss: 0.3334 - accuracy: 0.8635\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.8635\n",
            "Epoch 75/100\n",
            "250/250 [==============================] - 0s 955us/step - loss: 0.3333 - accuracy: 0.8648\n",
            "Epoch 76/100\n",
            "250/250 [==============================] - 0s 976us/step - loss: 0.3330 - accuracy: 0.8624\n",
            "Epoch 77/100\n",
            "250/250 [==============================] - 0s 979us/step - loss: 0.3332 - accuracy: 0.8643\n",
            "Epoch 78/100\n",
            "250/250 [==============================] - 0s 995us/step - loss: 0.3329 - accuracy: 0.8649\n",
            "Epoch 79/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8624\n",
            "Epoch 80/100\n",
            "250/250 [==============================] - 0s 997us/step - loss: 0.3329 - accuracy: 0.8627\n",
            "Epoch 81/100\n",
            "250/250 [==============================] - 0s 962us/step - loss: 0.3329 - accuracy: 0.8622\n",
            "Epoch 82/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.8650\n",
            "Epoch 83/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8637\n",
            "Epoch 84/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.8620\n",
            "Epoch 85/100\n",
            "250/250 [==============================] - 0s 956us/step - loss: 0.3328 - accuracy: 0.8630\n",
            "Epoch 86/100\n",
            "250/250 [==============================] - 0s 981us/step - loss: 0.3327 - accuracy: 0.8626\n",
            "Epoch 87/100\n",
            "250/250 [==============================] - 0s 978us/step - loss: 0.3325 - accuracy: 0.8621\n",
            "Epoch 88/100\n",
            "250/250 [==============================] - 0s 976us/step - loss: 0.3327 - accuracy: 0.8629\n",
            "Epoch 89/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3325 - accuracy: 0.8637\n",
            "Epoch 90/100\n",
            "250/250 [==============================] - 0s 909us/step - loss: 0.3325 - accuracy: 0.8635\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - 0s 962us/step - loss: 0.3327 - accuracy: 0.8645\n",
            "Epoch 92/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3328 - accuracy: 0.8636\n",
            "Epoch 93/100\n",
            "250/250 [==============================] - 0s 949us/step - loss: 0.3325 - accuracy: 0.8629\n",
            "Epoch 94/100\n",
            "250/250 [==============================] - 0s 947us/step - loss: 0.3324 - accuracy: 0.8654\n",
            "Epoch 95/100\n",
            "250/250 [==============================] - 0s 981us/step - loss: 0.3326 - accuracy: 0.8624\n",
            "Epoch 96/100\n",
            "250/250 [==============================] - 0s 952us/step - loss: 0.3324 - accuracy: 0.8629\n",
            "Epoch 97/100\n",
            "250/250 [==============================] - 0s 962us/step - loss: 0.3323 - accuracy: 0.8643\n",
            "Epoch 98/100\n",
            "250/250 [==============================] - 0s 930us/step - loss: 0.3323 - accuracy: 0.8636\n",
            "Epoch 99/100\n",
            "250/250 [==============================] - 0s 975us/step - loss: 0.3326 - accuracy: 0.8634\n",
            "Epoch 100/100\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9c7b914be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6EKJCbc0Kdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "be742752-e524-41b4-db44-c2f2e2369962"
      },
      "source": [
        "# Now we need to call the predict method to predict a certain customer's churn whether or not they leave the bank (the predict method takes double sq brackets [[]])\n",
        "\n",
        "ann.predict(sc.transform([[1,0,0,600,1,40,3,60000,2,1,1,50000]]))\n",
        "print(ann.predict(sc.transform([[1,0,0,600,1,40,3,60000,2,1,1,50000]])))\n",
        "print(ann.predict(sc.transform([[1,0,0,600,1,40,3,60000,2,1,1,50000]])) > 0.5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0208101]]\n",
            "[[False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd9RXl-F0Kgy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "ee70e3b7-c5f3-48bf-a871-46798c4bc602"
      },
      "source": [
        "# Now we need to predict the test data to see how the model performs. \n",
        "y_pred = ann.predict(X_test)\n",
        "\n",
        "# Makes it 1 if greater than 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)), 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " ...\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEnuwEyT0Kjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3d24f30a-9e17-415a-880d-5d53ca0734f7"
      },
      "source": [
        "# Create a confusion matrix to visualize the num of TP, TN, FP, FN\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1526   69]\n",
            " [ 204  201]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8635"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}